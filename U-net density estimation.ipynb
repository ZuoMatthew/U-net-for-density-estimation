{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-NET modified for density kernel estimation\n",
    "\n",
    "\n",
    "U-NET is a popular end-to-end segmentation net used in biomedical imaging [1]. Following a suggestion from a friend(Weidi Xie) I wanted to see if this network structure could be used to perform density estimation. I took the U-NET implementation from Weidi. With some small changes and some basic pre-processing I have been able to achieve close to state-of-the-art performance.\n",
    "\n",
    "#### Background:\n",
    "\n",
    "By using machine learning it is possible to learn a mapping from intensity to a density kernel representation of cells and objects. Through integration of the densities one can then predict the number of cells of objects in a scene without having to explicity segment the objects of interest. [2] To do this accurately, for typical images, it is necessary to characterise the image pixels by calculating long feature descriptors which can abstract image information in different ways and at different scales. Neural networks can handle large training sets and learn features which are tuned for specific applications. Many different networks can be applied to this problem however the current state-of-the-art for density kernel estimation in 2-D is a Fully Convolutional Regression Neural Network [3] which comprises of convolutional layers along with the normal activation and pooling layers, but then has upsampling layers which allow the output of the network to expand to be in parity with the input dimension. U-NET is a fully convolutional network also but it uses concatenations of layers and a highly symmetrical network design. I was interested to see if this could be directly applied to the density estimation and how well it would peform.\n",
    "\n",
    "#### Changes:\n",
    "\n",
    "-- The loss function used is 'mse' not 'cross-entropy'.\n",
    "\n",
    "-- As I am not performing segmentation so don't use softmax and use relu rather than sigmoid in the last activation layer.\n",
    "\n",
    "-- As mentioned in [2] it is important to scale the density kernels from 0.0-1.0 to something higher ( in this case 0.0-255.0) otherwise the network struggles to learn the very small target values produced by Gaussian convolution. These values are down-scaled subsequently to valid densities (0.0-1.0).\n",
    "\n",
    "#### Datasets:\n",
    "\n",
    "I used one some of a dataset which I have recieved from a collaborator (Caroline Scott). These are cells imaged using bright-field microscopy. We are interested in how many cells are present in each image.\n",
    "\n",
    "\n",
    "#### Bibliography:\n",
    "\n",
    "[1] Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer International Publishing, 2015.\n",
    "\n",
    "[2] Lempitsky, Victor, and Andrew Zisserman. \"Learning to count objects in images.\" Advances in Neural Information Processing Systems. 2010.\n",
    "\n",
    "[3] Xie, Weidi, J. Alison Noble, and Andrew Zisserman. \"Microscopy Cell Counting with Fully Convolutional Regression Networks.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #Ensures I only use one of my available GPUs.\n",
    "from random import Random\n",
    "import cPickle as pickle\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.models import Sequential, Model, load_model\n",
    "\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "#Uses 80% of the memory of one GPU, rather than all the memory of all the GPUs\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8 \n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "\n",
    "#Some other specialist functions which could easily be substituted.\n",
    "from PIL import Image #for image import and output\n",
    "from scipy import ndimage #for Gaussian filtering\n",
    "import cv2 #for padding function of image border.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from model import buildModel #additional functions\n",
    "from generator import ImageDataGenerator, split_the_images #additional functions\n",
    "\n",
    "\n",
    "\n",
    "class TestCallback(Callback):\n",
    "    \"\"\"This function reports back the progress of the learning.\"\"\"\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        \n",
    "def step_decay(epoch):\n",
    "    \"\"\"This sets up the various decays of the learning rate.\"\"\"\n",
    "    step = 16\n",
    "    num =  epoch // step\n",
    "   \n",
    "    if num >2:\n",
    "         lrate = 1e-6\n",
    "    else:\n",
    "        if num % 3 == 0:\n",
    "            lrate = 1e-3\n",
    "        elif num % 3 == 1:\n",
    "            lrate = 1e-4\n",
    "        elif num % 3:\n",
    "            lrate = 1e-5\n",
    "    print('Learning rate for epoch {} is {}.'.format(epoch+1, lrate))\n",
    "    return np.float(lrate)\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    \"\"\"This is the default U-NET loss function which I do not use.\"\"\"\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    smooth =1.0\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def get_unet(img_rows,img_cols):\n",
    "    return buildModel(input_dim = (img_rows,img_cols,1))\n",
    "\n",
    "def return_shuffled_im(train_size, test_size, input_seq):\n",
    "    \"\"\"returns shuffled indices for test and training data.\"\"\"\n",
    "    input_seq = input_seq.astype(np.int32)\n",
    "    image_pool_size = np.array(input_seq).shape\n",
    "    rand_seq = np.random.choice(input_seq,input_seq.shape[0],replace=True)\n",
    "\n",
    "    assert (train_size + test_size) <= image_pool_size,\\\n",
    "    'your combined test and train images exceeds the number of images in your input sequence.'\n",
    "\n",
    "    shuf_train_images = rand_seq[:train_size]\n",
    "    shuf_test_images = rand_seq[train_size:train_size+test_size]\n",
    "\n",
    "\n",
    "    return shuf_train_images, shuf_test_images\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for training algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model (1488, 128, 128, 1) 20.385\n",
      "Learning rate for epoch 1 is 0.001.\n",
      "Epoch 1/100\n",
      "8/9 [=========================>....] - ETA: 7s - loss: 1.1195 Epoch 00000: saving model to saved_models/keras_tf_exp_sdataset02s2_model_0.hdf5\n",
      "9/9 [==============================] - 502s - loss: 1.0884 - val_loss: 0.9660\n",
      "Learning rate for epoch 2 is 0.001.\n",
      "Epoch 2/100\n",
      "8/9 [=========================>....] - ETA: 8s - loss: 0.7716 Epoch 00001: saving model to saved_models/keras_tf_exp_sdataset02s2_model_0.hdf5\n",
      "9/9 [==============================] - 161s - loss: 0.7539 - val_loss: 0.9707\n",
      "Learning rate for epoch 3 is 0.001.\n",
      "Epoch 3/100\n",
      "8/9 [=========================>....] - ETA: 8s - loss: 0.6482 Epoch 00002: saving model to saved_models/keras_tf_exp_sdataset02s2_model_0.hdf5\n",
      "9/9 [==============================] - 165s - loss: 0.6139 - val_loss: 0.9693\n",
      "Learning rate for epoch 4 is 0.001.\n",
      "Epoch 4/100\n",
      "1/9 [==>...........................] - ETA: 69s - loss: 0.4484"
     ]
    }
   ],
   "source": [
    "#generate how many models.\n",
    "models_to_gen = 5\n",
    "\n",
    "for bt in range(0,models_to_gen):\n",
    "    \n",
    "    file_path = 'dataset02/' #Dataset to use.\n",
    "    data_store = {}\n",
    "    data_store['input'] = []\n",
    "    data_store['gt'] = []\n",
    "    data_store['dense'] = []\n",
    "    \n",
    "    #Parameters of fit.\n",
    "    in_hei = 96 #Size of each image patch\n",
    "    in_wid = 96 #Size of each image patch\n",
    "    mag = 16 #The padding margin to use\n",
    "    train_size = 62 #The number of training images.\n",
    "    test_size = 18 #The number of test images.\n",
    "    learning_rate =\"1e-5 see learning rate decay\"\n",
    "    batch_size = 16 #For stochastic gradient descent, the number of images in each batch.\n",
    "    nb_epoch = 100 #The number of epochs\n",
    "    samples_per_epoch = 150 #For stochastic gradient descent, the number of batches per epoch.\n",
    "    image_pool_size = train_size+test_size\n",
    "    input_seq = np.arange(0,image_pool_size)\n",
    "    sigma = 2.0 #Size of kernel representing the ground-truth density.\n",
    "    save_best_only = False\n",
    "    loss = 'mse' #I use mean square error in this case.\n",
    "   \n",
    "    decay = 0.0\n",
    "    \n",
    "    \n",
    "    #Experiment number should match filename\n",
    "    filepath=\"saved_models/keras_tf_exp_sdataset02s2_model_\"+str(bt)\n",
    "    \n",
    "    for i in range(0,image_pool_size):\n",
    "        n = str(i+1).zfill(3)\n",
    "        \n",
    "        #Open intensity image.\n",
    "        img = Image.open(file_path+n+'cells.png').getdata()\n",
    "        wid, hei = img.size\n",
    "        temp = np.array(img).reshape((hei,wid,3))[:,:,2].astype(np.float32)\n",
    "        \n",
    "        data_store['input'].append(temp)\n",
    "        \n",
    "        #Open ground-truth image.\n",
    "        img =  Image.open(file_path+n+'dots.png').getdata()\n",
    "        data_store['gt'].append(np.array(img).reshape((hei,wid))[:,:].astype(np.float64))\n",
    "        \n",
    "        #Filter ground-truth image to produce density kernel representation\n",
    "        data_store['dense'].append(ndimage.filters.gaussian_filter(data_store['gt'][i],sigma,mode='constant'))\n",
    "\n",
    "\n",
    "    train = []\n",
    "    gtdata = []\n",
    "\n",
    "    shuf_train_images, shuf_test_images = return_shuffled_im(train_size,test_size,input_seq)\n",
    "    \n",
    "    X_trainf = []\n",
    "    Y_trainf = []\n",
    "    X_testf = []\n",
    "    Y_testf = []\n",
    "\n",
    "    for i in shuf_train_images:\n",
    "        X_trainf.append(data_store['input'][i])\n",
    "        Y_trainf.append(data_store['dense'][i])\n",
    "    for i in shuf_test_images:\n",
    "        X_testf.append(data_store['input'][i])\n",
    "        Y_testf.append(data_store['dense'][i])\n",
    "\n",
    "    #X_trainf, X_testf, Y_trainf, Y_testf = train_test_split(data_store['input'],  data_store['dense'], train_size = train_size,test_size=18)\n",
    "    train_cut, train_gtdata_cut, images_per_image = split_the_images(X_trainf, Y_trainf,in_hei,in_wid,mag)\n",
    "    test_cut, test_gtdata_cut, images_per_image = split_the_images(X_testf, Y_testf,in_hei,in_wid,mag)\n",
    "    \n",
    "\n",
    "    X_train = np.array(train_cut)\n",
    "    Y_train = np.array(train_gtdata_cut)  \n",
    "    X_test = np.array(test_cut)\n",
    "    Y_test = np.array(test_gtdata_cut)\n",
    "\n",
    "\n",
    "    #Stores history of paramaters.\n",
    "    hist = keras.callbacks.History()\n",
    "\n",
    "\n",
    "    # combine generators into one which yields image and masks\n",
    "    #train_generator = zip(image_generator, mask_generator)\n",
    "    height = X_train.shape[2]\n",
    "    width = X_train.shape[3]\n",
    "    model = get_unet(height, width)\n",
    "\n",
    "    X_train = np.swapaxes(X_train,1,3)\n",
    "    X_train = np.swapaxes(X_train,2,1)\n",
    "    Y_train = np.swapaxes(Y_train,1,3)\n",
    "    Y_train = np.swapaxes(Y_train,2,1)\n",
    "    X_test = np.swapaxes(X_test,1,3)\n",
    "    X_test = np.swapaxes(X_test,2,1)\n",
    "    Y_test = np.swapaxes(Y_test,1,3)\n",
    "    Y_test = np.swapaxes(Y_test,2,1)\n",
    "   \n",
    "    \n",
    "    print 'fitting model',X_train.shape,np.max(X_train)\n",
    "    checkpoint = ModelCheckpoint(filepath+\".hdf5\",  monitor='loss', verbose=1, save_best_only=save_best_only, mode='min')\n",
    "\n",
    "\n",
    "    \n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center = False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center = False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization = False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization = False,  # divide each input by its std\n",
    "        zca_whitening = False,  # apply ZCA whitening\n",
    "        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range = 0.3,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range = 0.3,  # randomly shift images vertically (fraction of total height)\n",
    "        zoom_range = 0.3,\n",
    "        shear_range = 0.,\n",
    "        horizontal_flip = True,  # randomly flip images\n",
    "        vertical_flip = True,\n",
    "        fill_mode = 'constant',\n",
    "        dim_ordering = 'tf')  # randomly flip images\n",
    "\n",
    "\n",
    "    change_lr = LearningRateScheduler(step_decay)\n",
    "\n",
    "    callbacks_list = [checkpoint, change_lr]\n",
    "\n",
    "\n",
    "\n",
    "    hist = model.fit_generator(datagen.flow(X_train,Y_train,batch_size = batch_size), samples_per_epoch = samples_per_epoch, nb_epoch=nb_epoch, callbacks=callbacks_list, verbose=True, validation_data=(X_test, Y_test))\n",
    "    \n",
    "    #Store the paramters of the fit, along with the model and data preprocessing.\n",
    "    hist.history['parameters'] = {}\n",
    "    hist.history['parameters']['batch_size'] = batch_size\n",
    "    hist.history['parameters']['nb_epoch'] = nb_epoch\n",
    "    hist.history['parameters']['in_hei'] = in_hei\n",
    "    hist.history['parameters']['in_wid'] = in_wid\n",
    "    hist.history['parameters']['mag'] = mag\n",
    "    hist.history['parameters']['sigma'] = sigma\n",
    "    hist.history['parameters']['train_size'] = train_size\n",
    "    hist.history['parameters']['learning_rate'] = learning_rate\n",
    "    hist.history['parameters']['shuf_train_images'] = shuf_train_images\n",
    "    hist.history['parameters']['shuf_test_images'] = shuf_test_images\n",
    "    hist.history['parameters']['loss'] = loss\n",
    "    hist.history['parameters']['save_best_only'] = save_best_only\n",
    "    pickle.dump(hist.history,open(filepath+'.his',\"wb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inpu_arr': [96.548259897475603, 94.437600363062685, 78.179729460864777, 99.268087152154877, 80.368131661639282, 96.992953637689368, 93.850181568048768, 93.812998783661172, 98.916281612723438, 93.795125003377549, 93.875230967193716, 80.947793589766263, 90.262105397745515, 76.207503915211674, 95.205622060159428, 82.523915063049927, 87.119067184423869, 89.999418922519155], 'loss': [0.61385537683963776, 0.44317046403884885, 0.46911012977361677, 0.42480987161397932, 0.39507896155118943, 0.34107576310634613, 0.35019675642251968, 0.2892808221280575, 0.22749567180871963, 0.35392936468124392, 0.31145465672016143, 0.36326146870851517, 0.36998029649257658, 0.33873944059014321, 0.33421666622161866, 0.31594151556491851, 0.31144656166434287, 0.28976163566112517, 0.26960971504449843, 0.27353150099515916, 0.33533056676387785, 0.32221672981977462, 0.3706328496336937, 0.30823867917060854, 0.35346551835536955, 0.31134958118200301, 0.28328273743391036, 0.21871690228581428, 0.27633555680513383, 0.32340390384197237, 0.3452031150460243, 0.37501276433467867, 0.32246156930923464, 0.30780443996191026, 0.30409052520990371, 0.31137880310416222, 0.22144127190113067, 0.29387890994548799, 0.31453347206115723, 0.35720784962177277, 0.33553663939237593, 0.3145131215453148, 0.28772011548280718, 0.31099582761526107, 0.33375659659504892, 0.21868603900074959, 0.28214561790227888, 0.29430915713310241, 0.34998456239700315, 0.32691464573144913, 0.36417267918586732, 0.33409605845808982, 0.31470226794481276, 0.27983692660927773, 0.29242680072784422, 0.25508718267083169, 0.29877940565347672, 0.32510365098714827, 0.30763746276497839, 0.37663185000419619, 0.31735442355275156, 0.33387639075517656, 0.31086243093013766, 0.29316629394888877, 0.21988084763288498, 0.28666307181119921, 0.29682521373033521, 0.3296642005443573, 0.33390627205371859, 0.32465059459209444, 0.28970637917518616, 0.33032709211111067, 0.28580467775464058, 0.23748397976160049, 0.29638075008988379, 0.29209097698330877, 0.33405975699424745, 0.32274287268519403, 0.37678452730178835, 0.31616879925131797, 0.32842517346143724, 0.30317525714635851, 0.27809210792183875, 0.23986316025257109, 0.29076990038156508, 0.35435459315776824, 0.30825875401496888, 0.36308528035879134, 0.32017922624945638, 0.34987557828426363, 0.29694796949625013, 0.29135808870196345, 0.23001523762941362, 0.31027779579162595, 0.28297144621610643, 0.32362064421176912, 0.353620408475399, 0.35505774021148684, 0.29553230106830597, 0.32492395788431166], 'parameters': {'shuf_train_images': array([ 7,  2, 22, 46,  5, 17, 55, 47, 20, 60, 49, 17,  4, 66, 36, 25, 18,\n",
      "        7, 69, 42, 28, 74, 10, 41,  1, 76, 71,  3, 42, 77, 31, 54, 35, 35,\n",
      "       65, 21, 10, 30, 48, 78,  7, 47, 10, 34, 32,  0,  6, 52, 44, 10, 73,\n",
      "       59, 48, 56, 11,  0, 29,  5, 70, 57, 17, 61], dtype=int32), 'in_wid': 96, 'save_best_only': False, 'nb_epoch': 100, 'shuf_test_images': array([29, 73, 56, 30, 44, 41, 37, 24, 31, 37,  2, 74, 69, 61,  4,  3, 50,\n",
      "       61], dtype=int32), 'batch_size': 16, 'train_size': 62, 'mag': 16, 'loss': 'mse', 'in_hei': 96, 'sigma': 2.0, 'learning_rate': '1e-5 see learning rate decay'}, 'pred_arr': [416.07324035858153, 262.40356195707869, 14.070143342599291, 204.50321469737364, 424.91993731816535, 173.74958318512813, 180.90084669651537, 217.20525305238462, 178.93919697971421, 216.56256363685242, 404.14093041042526, 257.78328928972468, 503.43479917591401, 27.556341463912222, 124.9926185291927, 130.74153683940861, 393.65053359504572, 412.29064355027111], 'abs_err': [416.07324035858153, 262.40356195707869, 14.070143342599291, 204.50321469737364, 424.91993731816535, 173.74958318512813, 180.90084669651537, 217.20525305238462, 178.93919697971421, 216.56256363685242, 404.14093041042526, 257.78328928972468, 503.43479917591401, 27.556341463912222, 124.9926185291927, 130.74153683940861, 393.65053359504572, 412.29064355027111], 'perc_arr': [96.548259897475603, 94.437600363062685, 78.179729460864777, 99.268087152154877, 80.368131661639282, 96.992953637689368, 93.850181568048768, 93.812998783661172, 98.916281612723438, 93.795125003377549, 93.875230967193716, 80.947793589766263, 90.262105397745515, 76.207503915211674, 95.205622060159428, 82.523915063049927, 87.119067184423869, 89.999418922519155], 'val_loss': [1.1003939874504727, 1.1020866077289813, 1.1018424254531662, 1.1017449431259323, 1.1012347787101235, 1.1005816672135282, 1.1001149509274573, 1.1006861164717487, 1.0994755836479642, 1.0996219456299312, 1.0976071494665962, 1.0961355563874047, 1.0948123416380473, 1.0959948494306042, 1.0965537456623107, 1.0967093985010352, 1.0958200159689619, 1.0962439189251099, 1.0969354066204418, 1.0976974153373804, 1.0983859306477286, 1.0986773722149707, 1.0989411124200732, 1.0988715938119977, 1.0985793333017715, 1.0982733222621459, 1.0980438796665382, 1.0978922433806237, 1.097521772728888, 1.0970061929655019, 1.0954395383423954, 1.0935207828361955, 1.0914229939829696, 1.0887505057854232, 1.0844710068432268, 1.0780368485074077, 1.0673859572314002, 1.0518443491309881, 1.0286175260423787, 0.99790710174582076, 0.95746317745565812, 0.90877602504635302, 0.85751235855881258, 0.80473262592460271, 0.75560648932501118, 0.70774729235994593, 0.67123434806449545, 0.64341865145359878, 0.62185880467640586, 0.60536348423058239, 0.59318493437711839, 0.58360888298462943, 0.57636209599742738, 0.57013406373422459, 0.56532575036571531, 0.56189520951981342, 0.55963397617624311, 0.55791791968254578, 0.55667046010839172, 0.55562879790172537, 0.55483244882068705, 0.55415019812062383, 0.55350159353542105, 0.55294396152236946, 0.55249032347152627, 0.55221169586810803, 0.55207755406283665, 0.55202730458781679, 0.55199697355015409, 0.5520550898034815, 0.55210234186853524, 0.55207647131411008, 0.5520916141707588, 0.55196899503331498, 0.55196708423533924, 0.55190146914510818, 0.55201480406577941, 0.55206366142051089, 0.55209925670728643, 0.55208666519158411, 0.55207248677120169, 0.55207430926599987, 0.55200444437839369, 0.55202786600286213, 0.55218110150761079, 0.55212534632947707, 0.55207719274417122, 0.55208165246855334, 0.55211265112652819, 0.55209426568062214, 0.55212171520623898, 0.55202135560965093, 0.55189516647132464, 0.55189083351029289, 0.55188168578401764, 0.55190613169085101, 0.55183748314502068, 0.5519721484225657, 0.5520926885375822, 0.55201967588315404]}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Improper config format: {u'l2': 9.999999747378752e-05, u'name': u'L1L2Regularizer', u'l1': 0.0}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ffd5771e1983>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# load weights into new model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded model from disk\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mimported_pickle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.his'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No model found in config file.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;31m# set weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    311\u001b[0m                         \u001b[0;34m'Maybe you meant to use '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                         '`Sequential.from_config(config)`?')\n\u001b[0;32m--> 313\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/layers/__init__.pyc\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     52\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/utils/generic_utils.pyc\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    137\u001b[0m                 return cls.from_config(config['config'],\n\u001b[1;32m    138\u001b[0m                                        custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[0;32m--> 139\u001b[0;31m                                                            list(custom_objects.items())))\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m   2485\u001b[0m         \u001b[0;31m# First, we create all layers and enqueue nodes to be processed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2486\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2487\u001b[0;31m             \u001b[0mprocess_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2488\u001b[0m         \u001b[0;31m# Then we process nodes in order of layer depth.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2489\u001b[0m         \u001b[0;31m# Nodes that cannot yet be processed (if the inbound node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36mprocess_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m   2471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m             layer = deserialize_layer(layer_data,\n\u001b[0;32m-> 2473\u001b[0;31m                                       custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0mcreated_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/layers/__init__.pyc\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     52\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/utils/generic_utils.pyc\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    139\u001b[0m                                                            list(custom_objects.items())))\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;31m# Then `cls` may be a function returning a class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m   1250\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m         \"\"\"\n\u001b[0;32m-> 1252\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/layers/convolutional.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mkernel_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0mbias_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m    459\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/layers/convolutional.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rank, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_regularizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_regularizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_regularizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivity_regularizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivity_regularizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/regularizers.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(identifier)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'config'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/regularizers.pyc\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     66\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                                     printable_module_name='regularizer')\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/utils/generic_utils.pyc\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midentifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'class_name'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'config'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Improper config format: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mclass_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcustom_objects\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mclass_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Improper config format: {u'l2': 9.999999747378752e-05, u'name': u'L1L2Regularizer', u'l1': 0.0}"
     ]
    }
   ],
   "source": [
    "##Load in the paramters.\n",
    "\n",
    "model_path ='saved_models'\n",
    "experiment = 'keras_tf_exp_sdataset02s2_model_'\n",
    "\n",
    "imported_pickle = pickle.load(open(model_path+'/'+experiment+'0.his',\"rb\"))\n",
    "\n",
    "parameters = imported_pickle['parameters']\n",
    "file_path = 'dataset02/'\n",
    "data_store = {}\n",
    "data_store['input'] = []\n",
    "data_store['gt'] = []\n",
    "data_store['dense'] = []\n",
    "train = []\n",
    "gtdata = []\n",
    "in_hei = parameters['in_hei']\n",
    "in_wid = parameters['in_wid']\n",
    "mag = parameters['mag']\n",
    "learning_rate = parameters['learning_rate']\n",
    "#get_unet = parameters['unet']\n",
    "loss = parameters['loss'] \n",
    "save_best_only = parameters['save_best_only']\n",
    "\n",
    "\n",
    "\n",
    "#optimizer_fn = parameters['optimizer']\n",
    "#model_fn = parameters['model']\n",
    "loss = 'mse'\n",
    "sigma = parameters['sigma']\n",
    "#metrics = [accuracy_custom,accuracy_custom2]\n",
    "num_of_train = 19\n",
    "test_size = 18\n",
    "\n",
    "   \n",
    "#optimizer = optimizer_fn(lr=learning_rate,momentum=0.9,decay=0.1)\n",
    "\n",
    "##Load the images for testing.\n",
    "for i in range(0,num_of_train):\n",
    "    n = str(i+80).zfill(3)\n",
    "    \n",
    "    #Open intensity image.\n",
    "    img = Image.open(file_path+n+'cells.png').getdata()\n",
    "    wid,hei = img.size\n",
    "    temp = np.array(img).reshape((hei,wid,3))[:,:,2].astype(np.float32)\n",
    "    \n",
    "    data_store['input'].append(temp)\n",
    "    \n",
    "    #Open ground-truth image.\n",
    "    img =  Image.open(file_path+n+'dots.png').getdata()\n",
    "    data_store['gt'].append(np.array(img).reshape((hei,wid))[:,:].astype(np.float64))\n",
    "    \n",
    "    #Filter ground-truth image to produce density kernel representation\n",
    "    data_store['dense'].append(ndimage.filters.gaussian_filter(data_store['gt'][i],sigma,mode='constant'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_trainf, X_testf, Y_trainf, Y_testf = train_test_split(data_store['input'],  data_store['dense'], train_size =1,test_size=test_size)\n",
    "\n",
    "test_cut, test_gtdata_cut,images_per_image = split_the_images(X_testf, Y_testf,in_hei,in_wid,mag)\n",
    "\n",
    "X_test = np.array(test_cut)\n",
    "Y_test = np.array(test_gtdata_cut)  \n",
    "\n",
    "\n",
    "\n",
    "X_test = np.swapaxes(X_test,1,3)\n",
    "X_test = np.swapaxes(X_test,2,1)\n",
    "\n",
    "for vt in range(0,5):\n",
    "    filename=model_path+\"/\"+experiment+\"\"+str(vt)+\".hdf5\"\n",
    "    # load json and create model\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    # load weights into new model\n",
    "    loaded_model = load_model(filename)\n",
    "    print(\"Loaded model from disk\")\n",
    "    imported_pickle = pickle.load(open(model_path+'/'+experiment+\"\"+str(vt)+'.his',\"rb\"))\n",
    "\n",
    "    f = open(model_path+'/'+experiment+str(vt)+'out.txt', 'w+')\n",
    "\n",
    "    loss_print = imported_pickle['loss']\n",
    "    val_loss_print = imported_pickle['val_loss']\n",
    "    f.write('loss_pt\\tval_pt\\n')\n",
    "    for loss_pt, val_pt in zip(loss_print, val_loss_print):\n",
    "            f.write(str(loss_pt)+'\\t'+str(val_pt)+'\\n')\n",
    "    f.close()\n",
    "    #print 'final train loss', imported_pickle['loss'][-1]\n",
    "    #print 'final train accuracy', imported_pickle['accuracy_custom'][-1]\n",
    "     \n",
    "    # evaluate loaded model on test data\n",
    "    #loaded_model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    imgs_mask_test = loaded_model.predict(X_test, verbose=1)\n",
    "    #Switches format for tensor flow.\n",
    "    imgs_mask_test = np.swapaxes(imgs_mask_test,2,1)\n",
    "    imgs_mask_test = np.swapaxes(imgs_mask_test,1,3)\n",
    "    data_store['output'] = []\n",
    "    c =0\n",
    "\n",
    "    null,f_hei, f_wid = imgs_mask_test[0].shape\n",
    "    \n",
    "    #reconstruct the images.\n",
    "\n",
    "    while c < (imgs_mask_test.__len__()-images_per_image)+1:\n",
    "        out = np.zeros((hei,wid))\n",
    "        rows = 0\n",
    "        for rst in range(0,hei,in_hei):\n",
    "            rows +=1\n",
    "            cols = 0\n",
    "            d = np.floor(float(c)/float(images_per_image))\n",
    "            for cst in range(0,wid,in_wid):\n",
    "                cols +=1\n",
    "                img = imgs_mask_test[c]\n",
    "                c +=1\n",
    "                top = bottom = left = right = 16\n",
    "                ren = rst + in_hei\n",
    "                cen = cst + in_wid\n",
    "                if ren> hei:\n",
    "\n",
    "                    top = top + (ren-hei)\n",
    "                    ren = hei\n",
    "                if cen> wid:\n",
    "\n",
    "\n",
    "                    right = right + (cen-wid)\n",
    "                    cen = wid\n",
    "\n",
    "                out[rst:ren,cst:cen] = img[0,bottom:-top,left:-right]\n",
    "        \n",
    "        data_store['output'].append(out)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    #Output of performance metrics\n",
    "\n",
    "\n",
    "    pef = []\n",
    "    trk = []\n",
    "    abs_err = []\n",
    "    inpu_arr = []\n",
    "    pred_arr = []\n",
    "    perc_arr = []\n",
    "\n",
    "    print data_store['output'].__len__()\n",
    "    print test_size\n",
    "    #figsize(12,12)\n",
    "    for idx in range(0,test_size):\n",
    "        #figure()\n",
    "        inpu_img = Y_testf[idx]\n",
    "        pred_img = data_store['output'][idx]\n",
    "        n = str(idx)\n",
    "        b = n.zfill(3)\n",
    "        pickle.dump( inpu_img, open( \"out_imgs/inpu_img\"+b+\".p\", \"wb\" ) )\n",
    "        pickle.dump( pred_img, open( \"out_imgs/pred_img\"+b+\".p\", \"wb\" ) )\n",
    "        #subplot(1,2,1)\n",
    "        #imshow(inpu_img)\n",
    "        #subplot(1,2,2)\n",
    "        #imshow(pred_img)\n",
    "        inpu_sum = np.sum(inpu_img)/255.0\n",
    "        pred_sum = np.sum(pred_img)/255.0\n",
    "        #print 'img',idx\n",
    "        #print inpu_sum\n",
    "        #print pred_sum\n",
    "        #print abs(inpu_sum-pred_sum)\n",
    "        inpu_arr.append(inpu_sum)\n",
    "        pred_arr.append(pred_sum)\n",
    "        abs_err.append(abs(inpu_sum-pred_sum))\n",
    "        perc_arr.append((1-(abs(inpu_sum-pred_sum)/pred_sum))*100)\n",
    "        #print np.round((1-(abs(inpu_sum-pred_sum)/pred_sum))*100,0),'%'\n",
    "    print np.average(abs_err)\n",
    "    print np.average(inpu_arr)\n",
    "    print 'average pred',np.average(pred_arr)\n",
    "    print 'average perc',np.average(perc_arr)\n",
    "    imported_pickle['pred_arr'] = pred_arr\n",
    "    imported_pickle['perc_arr'] = perc_arr\n",
    "    imported_pickle['abs_err'] = pred_arr\n",
    "    imported_pickle['inpu_arr'] = perc_arr\n",
    "    #imported_pickle['img_inpu_arr'] = Y_testf\n",
    "    #imported_pickle['img_pred_arr'] = data_store['output']\n",
    "\n",
    "    pickle.dump(imported_pickle,open(model_path+'/'+experiment+\"\"+str(vt)+'.his',\"w\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
